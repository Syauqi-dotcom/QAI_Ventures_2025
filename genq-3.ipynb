{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ad284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end trainable quantum-kernel SVM with Qiskit + PyTorch\n",
    "# - Learns SVM params (beta, b) and quantum feature map params theta\n",
    "# - Backprop: parameter-shift through the kernel into the circuit\n",
    "import math, numpy as np, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import Statevector\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Toy data (binary labels ±1)\n",
    "# -----------------------------\n",
    "X, y = make_moons(n_samples=1000, noise=0.9, random_state=0)\n",
    "X = StandardScaler().fit_transform(X).astype(np.float32)\n",
    "y = 2.0*y.astype(np.float32) - 1.0  # {0,1} -> {-1,+1}\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "Xtr = torch.tensor(Xtr)            # [n_train, d]\n",
    "Xte = torch.tensor(Xte)            # [n_test, d]\n",
    "ytr = torch.tensor(ytr)            # [n_train]\n",
    "yte = torch.tensor(yte)            # [n_test]\n",
    "\n",
    "n_train, d = Xtr.shape\n",
    "n_qubits = d\n",
    "n_layers = 2   # feel free to increase (cost grows with params × kernel evals)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) A parameterized Qiskit feature map  |phi_theta(x)>\n",
    "#    - Fixed data encoders (RX(x_i), RZ(x_i^2))\n",
    "#    - Trainable single-qubit layers + RZZ ring per layer\n",
    "#    All trainables are pure rotation angles -> clean shift rule.\n",
    "# ----------------------------------------------------\n",
    "@dataclass\n",
    "class AnsatzShape:\n",
    "    n_layers: int\n",
    "    n_qubits: int\n",
    "\n",
    "    @property\n",
    "    def sizes(self):\n",
    "        # For each layer and qubit: RY(theta_y), RZ(theta_z)\n",
    "        # And an RZZ entangler per (layer, qubit) along a ring\n",
    "        return {\n",
    "            \"theta_y\": (self.n_layers, self.n_qubits),\n",
    "            \"theta_z\": (self.n_layers, self.n_qubits),\n",
    "            \"theta_zz\": (self.n_layers, self.n_qubits),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def total_params(self):\n",
    "        L, Q = self.n_layers, self.n_qubits\n",
    "        return L * Q * 3\n",
    "\n",
    "def unflatten_theta(theta_flat: np.ndarray, shape: AnsatzShape):\n",
    "    L, Q = shape.n_layers, shape.n_qubits\n",
    "    assert theta_flat.size == shape.total_params\n",
    "    k = 0\n",
    "    th_y = theta_flat[k:k+L*Q].reshape(L, Q); k += L*Q\n",
    "    th_z = theta_flat[k:k+L*Q].reshape(L, Q); k += L*Q\n",
    "    th_zz = theta_flat[k:k+L*Q].reshape(L, Q)\n",
    "    return th_y, th_z, th_zz\n",
    "\n",
    "def build_feature_map(x_vec: np.ndarray, theta_flat: np.ndarray, shape: AnsatzShape) -> QuantumCircuit:\n",
    "    \"\"\"Return a circuit preparing |phi_theta(x)> from |0...0>.\"\"\"\n",
    "    L, Q = shape.n_layers, shape.n_qubits\n",
    "    th_y, th_z, th_zz = unflatten_theta(theta_flat, shape)\n",
    "\n",
    "    qc = QuantumCircuit(Q)\n",
    "    # Fixed data encoding\n",
    "    for q in range(Q):\n",
    "        qc.rx(float(x_vec[q]), q)     # RX(x_i)\n",
    "        qc.rz(float(x_vec[q]**2), q)  # RZ(x_i^2)\n",
    "\n",
    "    # Trainable layers\n",
    "    for l in range(L):\n",
    "        for q in range(Q):\n",
    "            qc.ry(float(th_y[l, q]), q)\n",
    "            qc.rz(float(th_z[l, q]), q)\n",
    "        # Entangling ring\n",
    "        for q in range(Q):\n",
    "            r = (q + 1) % Q\n",
    "            qc.rzz(float(th_zz[l, q]), q, r)\n",
    "    return qc\n",
    "\n",
    "def statevector_from_circuit(qc: QuantumCircuit) -> np.ndarray:\n",
    "    return Statevector.from_instruction(qc).data  # complex vector (2^n,)\n",
    "\n",
    "def batch_states(X_np: np.ndarray, theta_np: np.ndarray, shape: AnsatzShape) -> np.ndarray:\n",
    "    \"\"\"Return array [N, 2^n] of complex statevectors for all samples.\"\"\"\n",
    "    return np.stack([statevector_from_circuit(build_feature_map(x, theta_np, shape))\n",
    "                     for x in X_np], axis=0)\n",
    "\n",
    "def kernel_from_states(S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"K_ij = |<phi_i|phi_j>|^2 from S [N, D].\"\"\"\n",
    "    G = S @ S.conj().T             # [N,N] complex Gram\n",
    "    K = np.abs(G)**2               # fidelity kernel\n",
    "    # Stabilizer to keep PSD and help optimization:\n",
    "    K += 1e-6 * np.eye(K.shape[0], dtype=K.dtype)\n",
    "    return K.real.astype(np.float32)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Autograd: Quantum kernel forward + parameter-shift back\n",
    "# ---------------------------------------------------------\n",
    "class QKernelShift(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X: torch.Tensor, theta: torch.Tensor, n_layers_q: int):\n",
    "        # Save static config\n",
    "        shape = AnsatzShape(n_layers=int(n_layers_q), n_qubits=X.shape[1])\n",
    "        ctx.shape = shape\n",
    "\n",
    "        # Compute kernel K(theta) on CPU with Qiskit\n",
    "        X_np = X.detach().cpu().numpy()\n",
    "        th_np = theta.detach().cpu().numpy()\n",
    "        S = batch_states(X_np, th_np, shape)\n",
    "        K = kernel_from_states(S)\n",
    "\n",
    "        # For backward\n",
    "        ctx.save_for_backward(X.detach(), theta.detach())\n",
    "        return torch.from_numpy(K)  # [N,N] float32\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        # dL/dtheta = sum_ij (dL/dK_ij) * (dK_ij/dtheta)\n",
    "        X, theta = ctx.saved_tensors\n",
    "        shape: AnsatzShape = ctx.shape\n",
    "        X_np = X.cpu().numpy()\n",
    "        th_np = theta.cpu().numpy().copy()\n",
    "        gK = grad_output.detach().cpu().numpy()\n",
    "\n",
    "        shift = math.pi/2\n",
    "        P = th_np.size\n",
    "        grad_theta = np.zeros_like(th_np, dtype=np.float64)\n",
    "\n",
    "        # Helper to compute K for a given theta\n",
    "        def K_of(theta_vec: np.ndarray) -> np.ndarray:\n",
    "            S = batch_states(X_np, theta_vec, shape)\n",
    "            return kernel_from_states(S)\n",
    "\n",
    "        # Precompute K at current theta? Not necessary for central diff, but optional.\n",
    "\n",
    "        # Parameter-shift loop (exact for single-parameter rotation gates)\n",
    "        for p in range(P):\n",
    "            th_plus = th_np.copy();   th_plus[p]  += shift\n",
    "            th_minus = th_np.copy();  th_minus[p] -= shift\n",
    "\n",
    "            Kp = K_of(th_plus)\n",
    "            Km = K_of(th_minus)\n",
    "            dK = 0.5*(Kp - Km)       # (f(theta+π/2) - f(theta-π/2))/2\n",
    "\n",
    "            grad_theta[p] = np.sum(gK * dK, dtype=np.float64)\n",
    "\n",
    "        # No gradients for X (not learning the raw inputs)\n",
    "        grad_X = None\n",
    "        return grad_X, torch.from_numpy(grad_theta.astype(np.float32)), None\n",
    "\n",
    "def quantum_kernel_matrix(X: torch.Tensor, theta: torch.Tensor, n_layers_q: int) -> torch.Tensor:\n",
    "    return QKernelShift.apply(X, theta, n_layers_q)  # [N,N]\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4) SVM-in-primal parameters (β in R^n, b∈R)\n",
    "# --------------------------------------------\n",
    "shape = AnsatzShape(n_layers=n_layers, n_qubits=n_qubits)\n",
    "theta = torch.nn.Parameter(0.3*torch.randn(shape.total_params))  # trainable feature map\n",
    "beta = torch.nn.Parameter(torch.zeros(n_train))                  # SVM coeffs\n",
    "b = torch.nn.Parameter(torch.zeros(()))                          # bias\n",
    "\n",
    "C = 5.0     # soft-margin weight\n",
    "lr = 0.05\n",
    "optim = torch.optim.Adam([theta, beta, b], lr=lr)\n",
    "\n",
    "def svm_primal_loss(K: torch.Tensor, y: torch.Tensor, beta: torch.Tensor, b: torch.Tensor, C: float):\n",
    "    # scores s = K β + b\n",
    "    s = K @ beta + b                  # [n]\n",
    "    hinge = torch.clamp(1.0 - y * s, min=0.0)\n",
    "    reg = 0.5 * (beta @ (K @ beta))   # 0.5 * β^T K β\n",
    "    return reg + C * hinge.mean(), s\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5) Training loop (backprop end-to-end)\n",
    "# -----------------------------------------\n",
    "for step in range(200):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    K_tr = quantum_kernel_matrix(Xtr, theta, n_layers)  # backpropagates via parameter-shift\n",
    "    loss, scores = svm_primal_loss(K_tr, ytr, beta, b, C)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if (step+1) % 40 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Evaluate cross-kernel K(Xte, Xtr) to classify test points\n",
    "            # (no gradient needed)\n",
    "            # Build states once per set for efficiency\n",
    "            Xtr_np = Xtr.cpu().numpy()\n",
    "            Xte_np = Xte.cpu().numpy()\n",
    "            th_np = theta.detach().cpu().numpy()\n",
    "            S_tr = batch_states(Xtr_np, th_np, shape)\n",
    "            S_te = batch_states(Xte_np, th_np, shape)\n",
    "            G_te_tr = S_te @ S_tr.conj().T\n",
    "            K_te_tr = (np.abs(G_te_tr)**2).astype(np.float32)\n",
    "\n",
    "            preds = np.sign(K_te_tr @ beta.detach().cpu().numpy() + b.detach().cpu().numpy())\n",
    "            acc = (preds.squeeze() == yte.cpu().numpy()).mean()\n",
    "        print(f\"step {step+1:3d} | loss {loss.item():.4f} | test acc {acc*100:.1f}%\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
