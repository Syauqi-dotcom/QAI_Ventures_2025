{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cfe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santana/MyStuff/genq/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# End-to-end trainable quantum-kernel SVM with Qiskit + PyTorch\n",
    "# - Learns SVM params (beta, b) and quantum feature map params theta\n",
    "# - Backprop: parameter-shift through the kernel into the circuit\n",
    "import math, numpy as np, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import Statevector\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# === Save processed dataset, splits, and pipeline ===\n",
    "import os, json\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feccd912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1867890/3542734375.py:5: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:    step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
      "0  M1979787155             0.0             0.0        0               0  \n",
      "1  M2044282225             0.0             0.0        0               0  \n",
      "2   C553264065             0.0             0.0        1               0  \n",
      "3    C38997010         21182.0             0.0        1               0  \n",
      "4  M1230701703             0.0             0.0        0               0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"Synthetic_Financial_datasets_log.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"sriharshaeedala/financial-fraud-detection-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e550ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Make everything numeric: cleaner + encoder ===\n",
    "import re, warnings, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _snake(s: str) -> str:\n",
    "    s = re.sub(r\"[^\\w]+\", \"_\", s.strip())\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s.lower()\n",
    "\n",
    "def standardize_colnames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    mapping = {c: _snake(str(c)) for c in df.columns}\n",
    "    df.rename(columns=mapping, inplace=True)\n",
    "    return df\n",
    "\n",
    "def detect_label(df: pd.DataFrame, label_hint: Optional[str] = None) -> Optional[str]:\n",
    "    if label_hint and label_hint in df.columns:\n",
    "        return label_hint\n",
    "    candidates = [\n",
    "        \"is_fraud\",\"fraud\",\"fraud_bool\",\"fraudulent\",\"class\",\"target\",\"label\",\"y\"\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: any boolean-ish column with 2 unique values named like fraud\n",
    "    for c in df.columns:\n",
    "        if \"fraud\" in c and df[c].nunique(dropna=True) <= 2:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def coerce_numeric_strings(s: pd.Series) -> pd.Series:\n",
    "    # strip currency, commas, spaces, and parentheses for negatives\n",
    "    # e.g., \"(1,234.50)\" -> -1234.50, \"$1,234\" -> 1234\n",
    "    x = s.astype(str)\n",
    "    x = x.str.strip()\n",
    "    x = x.str.replace(r\"\\(([^)]+)\\)\", r\"-\\1\", regex=True)\n",
    "    x = x.str.replace(r\"[^\\d\\.\\-eE]\", \"\", regex=True)\n",
    "    out = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def maybe_parse_datetime(col: pd.Series) -> Tuple[Optional[pd.Series], Dict[str, pd.Series]]:\n",
    "    dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
    "    ok_ratio = dt.notna().mean()\n",
    "    if ok_ratio < 0.5:\n",
    "        return None, {}\n",
    "    dt = dt.dt.tz_convert(\"UTC\")  # fixed tz baseline\n",
    "    feats = {\n",
    "        \"year\": dt.dt.year,\n",
    "        \"month\": dt.dt.month,\n",
    "        \"day\": dt.dt.day,\n",
    "        \"dayofweek\": dt.dt.dayofweek,\n",
    "        \"hour\": dt.dt.hour.fillna(0).astype(\"Int64\"),\n",
    "        \"dayofyear\": dt.dt.dayofyear,\n",
    "        \"is_month_end\": dt.dt.is_month_end.astype(\"Int8\"),\n",
    "        \"is_month_start\": dt.dt.is_month_start.astype(\"Int8\"),\n",
    "        \"is_weekend\": dt.dt.dayofweek.isin([5,6]).astype(\"Int8\"),\n",
    "        \"epoch_seconds\": (dt.view(\"int64\") // 10**9)\n",
    "    }\n",
    "    return dt, feats\n",
    "\n",
    "def split_feature_types(df: pd.DataFrame, max_onehot_cardinality: int = 30) -> Dict[str, List[str]]:\n",
    "    types = {\"numeric\": [], \"categorical_low\": [], \"categorical_high\": [], \"boolean\": []}\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        if pd.api.types.is_bool_dtype(s) or (s.dropna().isin([0,1,True,False]).all() and s.nunique(dropna=True)<=2):\n",
    "            types[\"boolean\"].append(c)\n",
    "        elif pd.api.types.is_numeric_dtype(s):\n",
    "            types[\"numeric\"].append(c)\n",
    "        elif pd.api.types.is_object_dtype(s) or pd.api.types.is_categorical_dtype(s):\n",
    "            nun = s.nunique(dropna=True)\n",
    "            (types[\"categorical_low\"] if nun <= max_onehot_cardinality else types[\"categorical_high\"]).append(c)\n",
    "        else:\n",
    "            # treat the rest (e.g., Int64/boolean with NA) conservatively\n",
    "            if s.nunique(dropna=True) <= max_onehot_cardinality:\n",
    "                types[\"categorical_low\"].append(c)\n",
    "            else:\n",
    "                types[\"categorical_high\"].append(c)\n",
    "    return types\n",
    "\n",
    "def drop_obvious_ids(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    id_like = [c for c in df.columns if re.search(r\"(?:^|_)(id|uuid|guid|hash|account|acct|customer|merchant)(?:_|$)\", c)]\n",
    "    return df.drop(columns=id_like, errors=\"ignore\"), id_like\n",
    "\n",
    "# ---------- main numericizer ----------\n",
    "@dataclass\n",
    "class Numericized:\n",
    "    X: np.ndarray\n",
    "    y: Optional[np.ndarray]\n",
    "    X_df: pd.DataFrame\n",
    "    feature_names: List[str]\n",
    "    label_name: Optional[str]\n",
    "    pipeline: ColumnTransformer\n",
    "    splits: Dict[str, List[str]]\n",
    "    dropped_cols: List[str]\n",
    "\n",
    "def make_all_numeric(\n",
    "    df_raw: pd.DataFrame,\n",
    "    label_hint: Optional[str] = None,\n",
    "    max_onehot_cardinality: int = 30,\n",
    "    clip_quantiles: Tuple[float,float] = (0.001, 0.999),\n",
    "    scale_numeric: bool = True\n",
    ") -> Numericized:\n",
    "    df = standardize_colnames(df_raw).copy()\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Try to parse datetimes & numeric-like strings in object columns\n",
    "    new_cols = {}\n",
    "    datetime_orig_cols = []\n",
    "    for c in list(df.columns):\n",
    "        if pd.api.types.is_object_dtype(df[c]):\n",
    "            # datetime?\n",
    "            _, dt_feats = maybe_parse_datetime(df[c])\n",
    "            if dt_feats:\n",
    "                datetime_orig_cols.append(c)\n",
    "                for k,v in dt_feats.items():\n",
    "                    new_cols[f\"{c}__{k}\"] = v\n",
    "                continue\n",
    "            # numeric-like strings?\n",
    "            coerced = coerce_numeric_strings(df[c])\n",
    "            if coerced.notna().mean() >= 0.5:\n",
    "                df[c] = coerced\n",
    "    if new_cols:\n",
    "        for k,v in new_cols.items():\n",
    "            df[k] = v\n",
    "        df.drop(columns=datetime_orig_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Convert 'yes/no', 'true/false' strings to booleans if present\n",
    "    for c in df.select_dtypes(include=\"object\").columns:\n",
    "        lc = df[c].str.lower()\n",
    "        mask_bool = lc.isin([\"true\",\"false\",\"yes\",\"no\",\"y\",\"n\",\"t\",\"f\"])\n",
    "        if mask_bool.mean() > 0.8:\n",
    "            df[c] = lc.map({\"true\":1,\"false\":0,\"yes\":1,\"no\":0,\"y\":1,\"n\":0,\"t\":1,\"f\":0}).astype(\"Int8\")\n",
    "\n",
    "    # Drop obvious ID/high-leakage identifiers\n",
    "    df, dropped_ids = drop_obvious_ids(df)\n",
    "\n",
    "    # Detect label (optional)\n",
    "    y_name = detect_label(df, label_hint)\n",
    "    y = None\n",
    "    if y_name:\n",
    "        y = df[y_name].copy()\n",
    "        # if label still not numeric, coerce\n",
    "        if not pd.api.types.is_numeric_dtype(y):\n",
    "            if y.dropna().isin([0,1]).all():\n",
    "                y = y.astype(int)\n",
    "            else:\n",
    "                # binary map if possible\n",
    "                if y.nunique(dropna=True)==2:\n",
    "                    y = pd.Series(pd.Categorical(y).codes, index=y.index)\n",
    "                else:\n",
    "                    # multi-class: ordinal codes\n",
    "                    y = pd.Series(pd.Categorical(y).codes, index=y.index)\n",
    "        df = df.drop(columns=[y_name])\n",
    "\n",
    "    # Final type split\n",
    "    splits = split_feature_types(df, max_onehot_cardinality=max_onehot_cardinality)\n",
    "\n",
    "    # Optional winsorization/clipping for numeric to tame outliers\n",
    "    if splits[\"numeric\"]:\n",
    "        q_lo = df[splits[\"numeric\"]].quantile(clip_quantiles[0])\n",
    "        q_hi = df[splits[\"numeric\"]].quantile(clip_quantiles[1])\n",
    "        df[splits[\"numeric\"]] = np.clip(df[splits[\"numeric\"]], q_lo, q_hi, axis=1)\n",
    "\n",
    "    # Build preprocessing\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        *([(\"scaler\", StandardScaler())] if scale_numeric else [])\n",
    "    ])\n",
    "\n",
    "    bool_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))  # booleans already 0/1 mostly\n",
    "    ])\n",
    "\n",
    "    # OneHot for low-card; Ordinal for high-card to avoid huge dimensionality\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    cat_low_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ])\n",
    "\n",
    "    cat_high_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if splits[\"numeric\"]: transformers.append((\"num\", num_pipe, splits[\"numeric\"]))\n",
    "    if splits[\"boolean\"]: transformers.append((\"bool\", bool_pipe, splits[\"boolean\"]))\n",
    "    if splits[\"categorical_low\"]: transformers.append((\"cat_low\", cat_low_pipe, splits[\"categorical_low\"]))\n",
    "    if splits[\"categorical_high\"]: transformers.append((\"cat_high\", cat_high_pipe, splits[\"categorical_high\"]))\n",
    "\n",
    "    pre = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=0.0)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"varth\", VarianceThreshold(threshold=0.0))\n",
    "    ])\n",
    "\n",
    "    X = pipeline.fit_transform(df)\n",
    "\n",
    "    # Construct feature names (best-effort)\n",
    "    feat_names = []\n",
    "    pre_fitted = pipeline.named_steps[\"pre\"]\n",
    "    for name, trans, cols in pre_fitted.transformers_:\n",
    "        if name == \"num\" or name == \"bool\":\n",
    "            feat_names.extend(cols)\n",
    "        elif name == \"cat_low\":\n",
    "            ohe_step = trans.named_steps[\"ohe\"]\n",
    "            cats = ohe_step.categories_\n",
    "            for c_name, cat_vals in zip(cols, cats):\n",
    "                feat_names.extend([f\"{c_name}__{str(cv)}\" for cv in cat_vals])\n",
    "        elif name == \"cat_high\":\n",
    "            feat_names.extend([f\"{c}__ordinal\" for c in cols])\n",
    "\n",
    "    # Align names with VarianceThreshold drop\n",
    "    kept_mask = pipeline.named_steps[\"varth\"].get_support()\n",
    "    if len(feat_names) == kept_mask.shape[0]:\n",
    "        feat_names = [n for n, keep in zip(feat_names, kept_mask) if keep]\n",
    "\n",
    "    X_df = pd.DataFrame(X, columns=[str(c) for c in feat_names])\n",
    "\n",
    "    y_arr = y.to_numpy() if y is not None else None\n",
    "\n",
    "    print(f\"[clean] Rows: {len(df_raw)} -> {len(df)} after dedupe\")\n",
    "    print(f\"[clean] Dropped ID-like cols: {dropped_ids}\")\n",
    "    print(f\"[types] numeric={len(splits['numeric'])}, boolean={len(splits['boolean'])}, \"\n",
    "          f\"cat_low={len(splits['categorical_low'])}, cat_high={len(splits['categorical_high'])}\")\n",
    "    print(f\"[encode] Final X shape: {X_df.shape}, Label: {y_name!r}\")\n",
    "\n",
    "    return Numericized(\n",
    "        X=X_df.to_numpy(),\n",
    "        y=y_arr,\n",
    "        X_df=X_df,\n",
    "        feature_names=list(X_df.columns),\n",
    "        label_name=y_name,\n",
    "        pipeline=pre_fitted,\n",
    "        splits=splits,\n",
    "        dropped_cols=dropped_ids\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd4592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1867890/1797049710.py:51: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1867890/1797049710.py:51: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1867890/1797049710.py:51: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1867890/1797049710.py:51: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1867890/1797049710.py:51: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1867890/1797049710.py:51: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean] Rows: 6362620 -> 6362620 after dedupe\n",
      "[clean] Dropped ID-like cols: []\n",
      "[types] numeric=8, boolean=1, cat_low=1, cat_high=0\n",
      "[encode] Final X shape: (6362620, 14), Label: 'isfraud'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Numericized' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------- usage ----------\u001b[39;00m\n\u001b[32m      2\u001b[39m num = make_all_numeric(df, label_hint=\u001b[38;5;28;01mNone\u001b[39;00m, max_onehot_cardinality=\u001b[32m30\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mnum\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mfinancial_fraud_numeric.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Numericized' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# ---------- usage ----------\n",
    "num = make_all_numeric(df, label_hint=None, max_onehot_cardinality=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.makedirs(\"processed\", exist_ok=True)\n",
    "\n",
    "# 1) Full processed table (features + optional label)\n",
    "proc = num.X_df.copy()\n",
    "if num.y is not None:\n",
    "    proc[num.label_name or \"label\"] = num.y\n",
    "\n",
    "# Parquet (smaller/faster) + CSV\n",
    "proc.to_parquet(\"processed/financial_fraud_processed.parquet\", index=False)  # pip install pyarrow if needed\n",
    "proc.to_csv(\"processed/financial_fraud_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = pd.read_csv(\"financial_fraud_numeric.csv\")\n",
    "num = num.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78037f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1866921/1797049710.py:51: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1866921/1797049710.py:51: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1866921/1797049710.py:51: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1866921/1797049710.py:51: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1866921/1797049710.py:51: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
      "/tmp/ipykernel_1866921/1797049710.py:51: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean] Rows: 6362620 -> 6362620 after dedupe\n",
      "[clean] Dropped ID-like cols: []\n",
      "[types] numeric=8, boolean=1, cat_low=1, cat_high=0\n",
      "[encode] Final X shape: (6362620, 14), Label: 'isfraud'\n",
      "Train shape: (5090096, 14) | Test shape: (1272524, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example: ready for train/test split (uses detected label if present)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = num.X\n",
    "y = num.y  # may be None if no label found\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y if y is not None else np.zeros(len(X)),  # dummy if no label\n",
    "    test_size=0.2, random_state=42, stratify=y if (y is not None and len(np.unique(y))>1) else None\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"| Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ad284",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m200\u001b[39m):\n\u001b[32m    177\u001b[39m     optim.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     K_tr = \u001b[43mquantum_kernel_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# backpropagates via parameter-shift\u001b[39;00m\n\u001b[32m    180\u001b[39m     loss, scores = svm_primal_loss(K_tr, ytr, beta, b, C)\n\u001b[32m    182\u001b[39m     loss.backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mquantum_kernel_matrix\u001b[39m\u001b[34m(X, theta, n_layers_q)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquantum_kernel_matrix\u001b[39m(X: torch.Tensor, theta: torch.Tensor, n_layers_q: \u001b[38;5;28mint\u001b[39m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQKernelShift\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers_q\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyStuff/genq/venv/lib/python3.12/site-packages/torch/autograd/function.py:581\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    580\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mQKernelShift.forward\u001b[39m\u001b[34m(ctx, X, theta, n_layers_q)\u001b[39m\n\u001b[32m    107\u001b[39m X_np = X.detach().cpu().numpy()\n\u001b[32m    108\u001b[39m th_np = theta.detach().cpu().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m S = \u001b[43mbatch_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mth_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m K = kernel_from_states(S)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# For backward\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mbatch_states\u001b[39m\u001b[34m(X_np, theta_np, shape)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_states\u001b[39m(X_np: np.ndarray, theta_np: np.ndarray, shape: AnsatzShape) -> np.ndarray:\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return array [N, 2^n] of complex statevectors for all samples.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.stack([\u001b[43mstatevector_from_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_feature_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m                      \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_np], axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mstatevector_from_circuit\u001b[39m\u001b[34m(qc)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstatevector_from_circuit\u001b[39m(qc: QuantumCircuit) -> np.ndarray:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStatevector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqc\u001b[49m\u001b[43m)\u001b[49m.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyStuff/genq/venv/lib/python3.12/site-packages/qiskit/quantum_info/states/statevector.py:780\u001b[39m, in \u001b[36mStatevector.from_instruction\u001b[39m\u001b[34m(cls, instruction)\u001b[39m\n\u001b[32m    778\u001b[39m init[\u001b[32m0\u001b[39m] = \u001b[32m1.0\u001b[39m\n\u001b[32m    779\u001b[39m vec = Statevector(init, dims=instruction.num_qubits * (\u001b[32m2\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStatevector\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_evolve_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyStuff/genq/venv/lib/python3.12/site-packages/qiskit/quantum_info/states/statevector.py:976\u001b[39m, in \u001b[36mStatevector._evolve_instruction\u001b[39m\u001b[34m(statevec, obj, qargs)\u001b[39m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    975\u001b[39m         new_qargs = [qargs[qubits[tup]] \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m instruction.qubits]\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m     \u001b[43mStatevector\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_evolve_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatevec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m.\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_qargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m statevec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyStuff/genq/venv/lib/python3.12/site-packages/qiskit/quantum_info/states/statevector.py:918\u001b[39m, in \u001b[36mStatevector._evolve_instruction\u001b[39m\u001b[34m(statevec, obj, qargs)\u001b[39m\n\u001b[32m    914\u001b[39m mat = Operator._instruction_to_matrix(obj)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    916\u001b[39m     \u001b[38;5;66;03m# Perform the composition and inplace update the current state\u001b[39;00m\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# of the operator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStatevector\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_evolve_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatevec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOperator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;66;03m# Special instruction types\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, Reset):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyStuff/genq/venv/lib/python3.12/site-packages/qiskit/quantum_info/states/statevector.py:891\u001b[39m, in \u001b[36mStatevector._evolve_operator\u001b[39m\u001b[34m(statevec, oper, qargs)\u001b[39m\n\u001b[32m    887\u001b[39m tensor_shape = tensor.shape\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Perform contraction\u001b[39;00m\n\u001b[32m    890\u001b[39m tensor = np.reshape(\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43moper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontract_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    892\u001b[39m     tensor_shape,\n\u001b[32m    893\u001b[39m )\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# Transpose back to  original subsystem spec and flatten\u001b[39;00m\n\u001b[32m    896\u001b[39m statevec._data = np.reshape(np.transpose(tensor, axes_inv), new_shape.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Toy data (binary labels ±1)\n",
    "# -----------------------------\n",
    "X = num.X\n",
    "y = num.y\n",
    "# X, y = make_moons(n_samples=1000, noise=0.9, random_state=0)\n",
    "X = StandardScaler().fit_transform(X).astype(np.float32)\n",
    "y = 2.0*y.astype(np.float32) - 1.0  # {0,1} -> {-1,+1}\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "Xtr = torch.tensor(Xtr)            # [n_train, d]\n",
    "Xte = torch.tensor(Xte)            # [n_test, d]\n",
    "ytr = torch.tensor(ytr)            # [n_train]\n",
    "yte = torch.tensor(yte)            # [n_test]\n",
    "\n",
    "n_train, d = Xtr.shape\n",
    "n_qubits = d\n",
    "n_layers = 2   # feel free to increase (cost grows with params × kernel evals)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) A parameterized Qiskit feature map  |phi_theta(x)>\n",
    "#    - Fixed data encoders (RX(x_i), RZ(x_i^2))\n",
    "#    - Trainable single-qubit layers + RZZ ring per layer\n",
    "#    All trainables are pure rotation angles -> clean shift rule.\n",
    "# ----------------------------------------------------\n",
    "@dataclass\n",
    "class AnsatzShape:\n",
    "    n_layers: int\n",
    "    n_qubits: int\n",
    "\n",
    "    @property\n",
    "    def sizes(self):\n",
    "        # For each layer and qubit: RY(theta_y), RZ(theta_z)\n",
    "        # And an RZZ entangler per (layer, qubit) along a ring\n",
    "        return {\n",
    "            \"theta_y\": (self.n_layers, self.n_qubits),\n",
    "            \"theta_z\": (self.n_layers, self.n_qubits),\n",
    "            \"theta_zz\": (self.n_layers, self.n_qubits),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def total_params(self):\n",
    "        L, Q = self.n_layers, self.n_qubits\n",
    "        return L * Q * 3\n",
    "\n",
    "def unflatten_theta(theta_flat: np.ndarray, shape: AnsatzShape):\n",
    "    L, Q = shape.n_layers, shape.n_qubits\n",
    "    assert theta_flat.size == shape.total_params\n",
    "    k = 0\n",
    "    th_y = theta_flat[k:k+L*Q].reshape(L, Q); k += L*Q\n",
    "    th_z = theta_flat[k:k+L*Q].reshape(L, Q); k += L*Q\n",
    "    th_zz = theta_flat[k:k+L*Q].reshape(L, Q)\n",
    "    return th_y, th_z, th_zz\n",
    "\n",
    "def build_feature_map(x_vec: np.ndarray, theta_flat: np.ndarray, shape: AnsatzShape) -> QuantumCircuit:\n",
    "    \"\"\"Return a circuit preparing |phi_theta(x)> from |0...0>.\"\"\"\n",
    "    L, Q = shape.n_layers, shape.n_qubits\n",
    "    th_y, th_z, th_zz = unflatten_theta(theta_flat, shape)\n",
    "\n",
    "    qc = QuantumCircuit(Q)\n",
    "    # Fixed data encoding\n",
    "    for q in range(Q):\n",
    "        qc.rx(float(x_vec[q]), q)     # RX(x_i)\n",
    "        qc.rz(float(x_vec[q]**2), q)  # RZ(x_i^2)\n",
    "\n",
    "    # Trainable layers\n",
    "    for l in range(L):\n",
    "        for q in range(Q):\n",
    "            qc.ry(float(th_y[l, q]), q)\n",
    "            qc.rz(float(th_z[l, q]), q)\n",
    "        # Entangling ring\n",
    "        for q in range(Q):\n",
    "            r = (q + 1) % Q\n",
    "            qc.rzz(float(th_zz[l, q]), q, r)\n",
    "    return qc\n",
    "\n",
    "def statevector_from_circuit(qc: QuantumCircuit) -> np.ndarray:\n",
    "    return Statevector.from_instruction(qc).data  # complex vector (2^n,)\n",
    "\n",
    "def batch_states(X_np: np.ndarray, theta_np: np.ndarray, shape: AnsatzShape) -> np.ndarray:\n",
    "    \"\"\"Return array [N, 2^n] of complex statevectors for all samples.\"\"\"\n",
    "    return np.stack([statevector_from_circuit(build_feature_map(x, theta_np, shape))\n",
    "                     for x in X_np], axis=0)\n",
    "\n",
    "def kernel_from_states(S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"K_ij = |<phi_i|phi_j>|^2 from S [N, D].\"\"\"\n",
    "    G = S @ S.conj().T             # [N,N] complex Gram\n",
    "    K = np.abs(G)**2               # fidelity kernel\n",
    "    # Stabilizer to keep PSD and help optimization:\n",
    "    K += 1e-6 * np.eye(K.shape[0], dtype=K.dtype)\n",
    "    return K.real.astype(np.float32)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Autograd: Quantum kernel forward + parameter-shift back\n",
    "# ---------------------------------------------------------\n",
    "class QKernelShift(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X: torch.Tensor, theta: torch.Tensor, n_layers_q: int):\n",
    "        # Save static config\n",
    "        shape = AnsatzShape(n_layers=int(n_layers_q), n_qubits=X.shape[1])\n",
    "        ctx.shape = shape\n",
    "\n",
    "        # Compute kernel K(theta) on CPU with Qiskit\n",
    "        X_np = X.detach().cpu().numpy()\n",
    "        th_np = theta.detach().cpu().numpy()\n",
    "        S = batch_states(X_np, th_np, shape)\n",
    "        K = kernel_from_states(S)\n",
    "\n",
    "        # For backward\n",
    "        ctx.save_for_backward(X.detach(), theta.detach())\n",
    "        return torch.from_numpy(K)  # [N,N] float32\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        # dL/dtheta = sum_ij (dL/dK_ij) * (dK_ij/dtheta)\n",
    "        X, theta = ctx.saved_tensors\n",
    "        shape: AnsatzShape = ctx.shape\n",
    "        X_np = X.cpu().numpy()\n",
    "        th_np = theta.cpu().numpy().copy()\n",
    "        gK = grad_output.detach().cpu().numpy()\n",
    "\n",
    "        shift = math.pi/2\n",
    "        P = th_np.size\n",
    "        grad_theta = np.zeros_like(th_np, dtype=np.float64)\n",
    "\n",
    "        # Helper to compute K for a given theta\n",
    "        def K_of(theta_vec: np.ndarray) -> np.ndarray:\n",
    "            S = batch_states(X_np, theta_vec, shape)\n",
    "            return kernel_from_states(S)\n",
    "\n",
    "        # Precompute K at current theta? Not necessary for central diff, but optional.\n",
    "\n",
    "        # Parameter-shift loop (exact for single-parameter rotation gates)\n",
    "        for p in range(P):\n",
    "            th_plus = th_np.copy();   th_plus[p]  += shift\n",
    "            th_minus = th_np.copy();  th_minus[p] -= shift\n",
    "\n",
    "            Kp = K_of(th_plus)\n",
    "            Km = K_of(th_minus)\n",
    "            dK = 0.5*(Kp - Km)       # (f(theta+π/2) - f(theta-π/2))/2\n",
    "\n",
    "            grad_theta[p] = np.sum(gK * dK, dtype=np.float64)\n",
    "\n",
    "        # No gradients for X (not learning the raw inputs)\n",
    "        grad_X = None\n",
    "        return grad_X, torch.from_numpy(grad_theta.astype(np.float32)), None\n",
    "\n",
    "def quantum_kernel_matrix(X: torch.Tensor, theta: torch.Tensor, n_layers_q: int) -> torch.Tensor:\n",
    "    return QKernelShift.apply(X, theta, n_layers_q)  # [N,N]\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4) SVM-in-primal parameters (β in R^n, b∈R)\n",
    "# --------------------------------------------\n",
    "shape = AnsatzShape(n_layers=n_layers, n_qubits=n_qubits)\n",
    "theta = torch.nn.Parameter(0.3*torch.randn(shape.total_params))  # trainable feature map\n",
    "beta = torch.nn.Parameter(torch.zeros(n_train))                  # SVM coeffs\n",
    "b = torch.nn.Parameter(torch.zeros(()))                          # bias\n",
    "\n",
    "C = 5.0     # soft-margin weight\n",
    "lr = 0.05\n",
    "optim = torch.optim.Adam([theta, beta, b], lr=lr)\n",
    "\n",
    "def svm_primal_loss(K: torch.Tensor, y: torch.Tensor, beta: torch.Tensor, b: torch.Tensor, C: float):\n",
    "    # scores s = K β + b\n",
    "    s = K @ beta + b                  # [n]\n",
    "    hinge = torch.clamp(1.0 - y * s, min=0.0)\n",
    "    reg = 0.5 * (beta @ (K @ beta))   # 0.5 * β^T K β\n",
    "    return reg + C * hinge.mean(), s\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5) Training loop (backprop end-to-end)\n",
    "# -----------------------------------------\n",
    "for step in range(200):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    K_tr = quantum_kernel_matrix(Xtr, theta, n_layers)  # backpropagates via parameter-shift\n",
    "    loss, scores = svm_primal_loss(K_tr, ytr, beta, b, C)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if (step+1) % 40 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Evaluate cross-kernel K(Xte, Xtr) to classify test points\n",
    "            # (no gradient needed)\n",
    "            # Build states once per set for efficiency\n",
    "            Xtr_np = Xtr.cpu().numpy()\n",
    "            Xte_np = Xte.cpu().numpy()\n",
    "            th_np = theta.detach().cpu().numpy()\n",
    "            S_tr = batch_states(Xtr_np, th_np, shape)\n",
    "            S_te = batch_states(Xte_np, th_np, shape)\n",
    "            G_te_tr = S_te @ S_tr.conj().T\n",
    "            K_te_tr = (np.abs(G_te_tr)**2).astype(np.float32)\n",
    "\n",
    "            preds = np.sign(K_te_tr @ beta.detach().cpu().numpy() + b.detach().cpu().numpy())\n",
    "            acc = (preds.squeeze() == yte.cpu().numpy()).mean()\n",
    "        print(f\"step {step+1:3d} | loss {loss.item():.4f} | test acc {acc*100:.1f}%\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
