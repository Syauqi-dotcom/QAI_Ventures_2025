{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73045ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qiskit implementation: Fisher ratio loss for fidelity kernels (QC-computable)\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "This script trains the parameters θ of a parametric quantum feature map U_θ(x)\n",
    "by **maximizing the Fisher ratio** of the induced *fidelity kernel*\n",
    "K_ij = |⟨ψ_θ(x_i)|ψ_θ(x_j)⟩|^2, computed via an **echo overlap circuit**\n",
    "(U_θ(x_j)^† U_θ(x_i)) and measured with a Sampler (or Aer fallback).\n",
    "\n",
    "Highlights\n",
    "- Purely fidelity-based losses: only needs probabilities of measuring |0…0⟩.\n",
    "- Parameter-shift gradients **on hardware**: each ∂K_ij/∂θ_k comes from 4\n",
    "  shifted fidelity evaluations on the echo circuit (±π/2 applied to either\n",
    "  the \"i\" or the \"j\" side of the overlap).\n",
    "- Works mini-batch: compute loss from batch sub-kernels\n",
    "- Two-class & multi-class Fisher criteria\n",
    "\n",
    "Dependencies\n",
    "- qiskit >= 0.46 (Terra 1.x) recommended\n",
    "- qiskit-aer (optional but recommended for local simulation)\n",
    "\n",
    "Quick start\n",
    "-----------\n",
    "1) Prepare your data X (shape [N, d]) and labels y (ints 0..C-1).\n",
    "2) Run `python qiskit_fisher_fidelity_training.py` to see a demo with\n",
    "   synthetic data on an Aer Sampler.\n",
    "3) To plug your own feature map, implement `build_feature_map()` with your\n",
    "   U_θ(x) and update `FeatureMapConfig` as needed.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter, ParameterVector\n",
    "\n",
    "# Try modern Sampler first; fall back to Aer primitives or AerSimulator counts\n",
    "try:\n",
    "    from qiskit_aer.primitives import Sampler as AerSampler\n",
    "    _HAVE_AER_SAMPLER = True\n",
    "except Exception:\n",
    "    _HAVE_AER_SAMPLER = False\n",
    "\n",
    "try:\n",
    "    from qiskit.primitives import Sampler\n",
    "    _HAVE_TERRA_SAMPLER = True\n",
    "except Exception:\n",
    "    _HAVE_TERRA_SAMPLLER = False\n",
    "\n",
    "\n",
    "# ------------------------------ Feature Map ------------------------------ #\n",
    "@dataclass\n",
    "class FeatureMapConfig:\n",
    "    n_qubits: int\n",
    "    data_dim: int\n",
    "    n_layers: int = 2\n",
    "    name: str = \"EchoFidelityDefault\"\n",
    "\n",
    "\n",
    "def build_feature_map(cfg: FeatureMapConfig,\n",
    "                      theta_i: ParameterVector,\n",
    "                      x_i: ParameterVector) -> QuantumCircuit:\n",
    "    \"\"\"Construct a simple, expressive, trainable feature map U_θ(x).\n",
    "\n",
    "    Layout:\n",
    "      For L layers:\n",
    "        - Data encoding (RZ(α x), RX(β x) with learned α,β via θ)\n",
    "        - Trainable single-qubit Ry/Rz\n",
    "        - Linear entanglement with CZ + trainable Rz on controls\n",
    "\n",
    "    Parameters are consumed left-to-right, top-to-bottom for determinism.\n",
    "    This is just a **reasonable default**—replace with your physics-informed\n",
    "    ansatz if you have one.\n",
    "    \"\"\"\n",
    "    n = cfg.n_qubits\n",
    "    L = cfg.n_layers\n",
    "    qc = QuantumCircuit(n, name=f\"U_{cfg.name}\")\n",
    "\n",
    "    # We'll allocate parameters as follows per layer:\n",
    "    # per-qubit: [alpha_k, beta_k, ry_k, rz_k] → 4*n\n",
    "    # entanglers between q and q+1: [phi_q] → (n-1)\n",
    "    # total per-layer: 4*n + (n-1)\n",
    "    per_layer = 4 * n + (n - 1)\n",
    "    assert len(theta_i) >= L * per_layer, (\n",
    "        f\"theta length {len(theta_i)} < required {L*per_layer}\")\n",
    "\n",
    "    th_idx = 0\n",
    "    for layer in range(L):\n",
    "        # Data encoding with learned scalings (alpha, beta)\n",
    "        for q in range(n):\n",
    "            alpha = theta_i[th_idx]; th_idx += 1\n",
    "            beta  = theta_i[th_idx]; th_idx += 1\n",
    "            # Map data dim -> qubits by modulo (simple, works well in practice)\n",
    "            xparam = x_i[q % cfg.data_dim]\n",
    "            qc.rz(alpha * xparam, q)\n",
    "            qc.rx(beta  * xparam, q)\n",
    "\n",
    "        # Trainable single-qubit block\n",
    "        for q in range(n):\n",
    "            ry = theta_i[th_idx]; th_idx += 1\n",
    "            rz = theta_i[th_idx]; th_idx += 1\n",
    "            qc.ry(ry, q)\n",
    "            qc.rz(rz, q)\n",
    "\n",
    "        # Linear entanglement\n",
    "        for q in range(n - 1):\n",
    "            phi = theta_i[th_idx]; th_idx += 1\n",
    "            qc.cz(q, q + 1)\n",
    "            qc.rz(phi, q + 1)\n",
    "\n",
    "    return qc\n",
    "\n",
    "\n",
    "# --------------------------- Echo overlap circuit ------------------------ #\n",
    "@dataclass\n",
    "class EchoCircuit:\n",
    "    cfg: FeatureMapConfig\n",
    "    theta_i: ParameterVector\n",
    "    x_i: ParameterVector\n",
    "    theta_j: ParameterVector\n",
    "    x_j: ParameterVector\n",
    "    circuit: QuantumCircuit\n",
    "\n",
    "\n",
    "def build_echo_circuit(cfg: FeatureMapConfig) -> EchoCircuit:\n",
    "    \"\"\"Build parameterized echo circuit for fidelity F_ij = |⟨ψ_i|ψ_j⟩|^2.\n",
    "\n",
    "    We construct U_θi(x_i) followed by [U_θj(x_j)]^†, then measure |0..0⟩.\n",
    "    Separate ParameterVectors (θi, θj, x_i, x_j) let us shift parameters\n",
    "    on one side at a time for parameter-shift gradients.\n",
    "    \"\"\"\n",
    "    n = cfg.n_qubits\n",
    "    theta_i = ParameterVector(\"theta_i\", length=(4 * n + (n - 1)) * cfg.n_layers)\n",
    "    theta_j = ParameterVector(\"theta_j\", length=len(theta_i))\n",
    "    x_i = ParameterVector(\"x_i\", length=cfg.data_dim)\n",
    "    x_j = ParameterVector(\"x_j\", length=cfg.data_dim)\n",
    "\n",
    "    U_i = build_feature_map(cfg, theta_i, x_i)\n",
    "    U_j = build_feature_map(cfg, theta_j, x_j)\n",
    "\n",
    "    qc = QuantumCircuit(n, name=\"EchoOverlap\")\n",
    "    qc.compose(U_i, inplace=True)\n",
    "    qc.compose(U_j.inverse(), inplace=True)\n",
    "    qc.measure_all()\n",
    "\n",
    "    return EchoCircuit(cfg, theta_i, x_i, theta_j, x_j, qc)\n",
    "\n",
    "\n",
    "# --------------------------- Sampler helper ------------------------------ #\n",
    "class ProbZeroEstimator:\n",
    "    \"\"\"Helper to get Pr(0…0) from a circuit with bound parameters.\n",
    "\n",
    "    Uses Sampler if available; otherwise tries Aer Sampler; finally falls back\n",
    "    to qasm_simulator via transpile+run. The interface exposes a single method\n",
    "    `batch_prob_zero(circuits, param_values)` returning a list of probabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, shots: Optional[int] = 4096):\n",
    "        self.shots = shots\n",
    "        self._mode = None\n",
    "        self._sampler = None\n",
    "\n",
    "        if _HAVE_AER_SAMPLER:\n",
    "            try:\n",
    "                self._sampler = AerSampler(shots=shots)\n",
    "                self._mode = \"aer_sampler\"\n",
    "                return\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if _HAVE_TERRA_SAMPLER:\n",
    "            try:\n",
    "                self._sampler = Sampler()\n",
    "                self._mode = \"terra_sampler\"\n",
    "                return\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Last resort: AerSimulator QASM counts\n",
    "        try:\n",
    "            from qiskit_aer import Aer\n",
    "            from qiskit import transpile\n",
    "            self._backend = Aer.get_backend(\"qasm_simulator\")\n",
    "            self._transpile = transpile\n",
    "            self._mode = \"qasm\"\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                \"No available Sampler or Aer qasm_simulator found.\\n\"\n",
    "                \"Install qiskit-aer or use a provider Sampler.\") from e\n",
    "\n",
    "    def batch_prob_zero(self,\n",
    "                        circuits: List[QuantumCircuit],\n",
    "                        param_values: List[Dict[Parameter, float]]) -> List[float]:\n",
    "        assert len(circuits) == len(param_values)\n",
    "        probs: List[float] = []\n",
    "\n",
    "        if self._mode in {\"aer_sampler\", \"terra_sampler\"}:\n",
    "            # Sampler path\n",
    "            # Group into a single run for efficiency\n",
    "            jobs = self._sampler.run(circuits=circuits,\n",
    "                                     parameter_values=[list(pv.values()) for pv in param_values],\n",
    "                                     parameter_binds=param_values if hasattr(self._sampler, 'run') else None,\n",
    "                                     shots=self.shots)\n",
    "            res = jobs.result()\n",
    "            # API differences: res.quasi_dists or .quasi_dists\n",
    "            dists = getattr(res, \"quasi_dists\", None)\n",
    "            if dists is None:\n",
    "                dists = res.quasi_dists\n",
    "            for i, dist in enumerate(dists):\n",
    "                n = circuits[i].num_qubits\n",
    "                key = \"0\" * n\n",
    "                p0 = float(dist.get(key, 0.0))\n",
    "                probs.append(p0)\n",
    "            return probs\n",
    "\n",
    "        # qasm path\n",
    "        from qiskit import execute\n",
    "        for c, bind in zip(circuits, param_values):\n",
    "            cb = c.bind_parameters(bind)\n",
    "            tc = self._transpile(cb, self._backend)\n",
    "            job = execute(tc, backend=self._backend, shots=self.shots)\n",
    "            counts = job.result().get_counts()\n",
    "            n = c.num_qubits\n",
    "            p0 = counts.get(\"0\" * n, 0) / self.shots\n",
    "            probs.append(float(p0))\n",
    "        return probs\n",
    "\n",
    "\n",
    "# ----------------------- Fidelity and gradients -------------------------- #\n",
    "class FidelityKernel:\n",
    "    \"\"\"Compute fidelities K_ij via echo circuits and parameter-shift grads.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: FeatureMapConfig, shots: Optional[int] = 4096):\n",
    "        self.cfg = cfg\n",
    "        self.echo = build_echo_circuit(cfg)\n",
    "        self.estimator = ProbZeroEstimator(shots=shots)\n",
    "\n",
    "    def _bind(self,\n",
    "              x_i: Sequence[float], theta_i: Sequence[float],\n",
    "              x_j: Sequence[float], theta_j: Sequence[float]) -> Dict[Parameter, float]:\n",
    "        bind = {}\n",
    "        # Data\n",
    "        for p, v in zip(self.echo.x_i, x_i):\n",
    "            bind[p] = float(v)\n",
    "        for p, v in zip(self.echo.x_j, x_j):\n",
    "            bind[p] = float(v)\n",
    "        # Params\n",
    "        for p, v in zip(self.echo.theta_i, theta_i):\n",
    "            bind[p] = float(v)\n",
    "        for p, v in zip(self.echo.theta_j, theta_j):\n",
    "            bind[p] = float(v)\n",
    "        return bind\n",
    "\n",
    "    def fidelity(self,\n",
    "                 x_i: Sequence[float], x_j: Sequence[float],\n",
    "                 theta: Sequence[float]) -> float:\n",
    "        \"\"\"Compute F_ij with θ_i = θ_j = θ.\"\"\"\n",
    "        bind = self._bind(x_i, theta, x_j, theta)\n",
    "        p0 = self.estimator.batch_prob_zero([self.echo.circuit], [bind])[0]\n",
    "        return float(p0)\n",
    "\n",
    "    def fidelities(self,\n",
    "                   pairs: List[Tuple[int, int]],\n",
    "                   X: np.ndarray,\n",
    "                   theta: Sequence[float]) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"Batch compute fidelities for selected index pairs (i,j).\"\"\"\n",
    "        circs, binds = [], []\n",
    "        for i, j in pairs:\n",
    "            bind = self._bind(X[i], theta, X[j], theta)\n",
    "            binds.append(bind)\n",
    "            circs.append(self.echo.circuit)\n",
    "        probs = self.estimator.batch_prob_zero(circs, binds)\n",
    "        return {(i, j): float(p) for (i, j), p in zip(pairs, probs)}\n",
    "\n",
    "    # ----- Parameter-shift gradients ----- #\n",
    "    def fidelity_param_shift_grad(self,\n",
    "                                  x_i: Sequence[float], x_j: Sequence[float],\n",
    "                                  theta: Sequence[float], k: int,\n",
    "                                  shift: float = math.pi / 2.0) -> float:\n",
    "        \"\"\"∂_θk F_ij using 4 shifted echo evaluations (shift on i- and j-sides).\"\"\"\n",
    "        theta = np.asarray(theta, dtype=float)\n",
    "        th_plus = theta.copy(); th_minus = theta.copy()\n",
    "\n",
    "        # Shift on the \"i\" side (θ_i)\n",
    "        th_plus[k] += shift\n",
    "        th_minus[k] -= shift\n",
    "        bind_p_i = self._bind(x_i, th_plus, x_j, theta)\n",
    "        bind_m_i = self._bind(x_i, th_minus, x_j, theta)\n",
    "\n",
    "        # Shift on the \"j\" side (θ_j)\n",
    "        bind_p_j = self._bind(x_i, theta, x_j, th_plus)\n",
    "        bind_m_j = self._bind(x_i, theta, x_j, th_minus)\n",
    "\n",
    "        circs = [self.echo.circuit] * 4\n",
    "        binds = [bind_p_i, bind_m_i, bind_p_j, bind_m_j]\n",
    "        p0_p_i, p0_m_i, p0_p_j, p0_m_j = self.estimator.batch_prob_zero(circs, binds)\n",
    "        return 0.5 * ((p0_p_i - p0_m_i) + (p0_p_j - p0_m_j))\n",
    "\n",
    "    def dK_dtheta(\n",
    "        self,\n",
    "        pairs: List[Tuple[int, int]],\n",
    "        X: np.ndarray,\n",
    "        theta: Sequence[float],\n",
    "        param_indices: Optional[Sequence[int]] = None,\n",
    "    ) -> Dict[int, float]:\n",
    "        \"\"\"Compute ∑_{(i,j)∈pairs} w_{ij} ∂K_ij/∂θ for each θ, given weights later.\n",
    "\n",
    "        Returns a dict mapping k → a callable contribution; in practice we\n",
    "        compute all ∂K_ij/∂θ_k and cache them so the trainer can combine with\n",
    "        dL/dK weights. To save shots, pass a subset of `param_indices`.\n",
    "        \"\"\"\n",
    "        if param_indices is None:\n",
    "            param_indices = list(range(len(theta)))\n",
    "        grads: Dict[int, Dict[Tuple[int, int], float]] = {k: {} for k in param_indices}\n",
    "        for (i, j) in pairs:\n",
    "            for k in param_indices:\n",
    "                g = self.fidelity_param_shift_grad(X[i], X[j], theta, k)\n",
    "                grads[k][(i, j)] = g\n",
    "        # Pack as k → (pair → grad)\n",
    "        return grads\n",
    "\n",
    "\n",
    "# ------------------------ Fisher loss and utilities ---------------------- #\n",
    "@dataclass\n",
    "class FisherLossOut:\n",
    "    loss: float\n",
    "    SB: float\n",
    "    SW: float\n",
    "    B_mat: np.ndarray\n",
    "    W_mat: np.ndarray\n",
    "\n",
    "\n",
    "def fisher_mats(y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Build B and W matrices such that:\n",
    "       tr(S_B)=⟨K,B⟩_F and tr(S_W)=⟨K,W⟩_F.\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    classes = np.unique(y)\n",
    "    B = np.zeros((n, n), dtype=float)\n",
    "    W = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    one = np.ones((n, n), dtype=float) / n\n",
    "    for c in classes:\n",
    "        idx = np.where(y == c)[0]\n",
    "        b = len(idx)\n",
    "        e = np.zeros((n, 1)); e[idx, 0] = 1.0\n",
    "        A_c = (e @ e.T) / b\n",
    "        P_c = np.diagflat(e)\n",
    "        B += A_c\n",
    "        W += (P_c - A_c)\n",
    "    B -= one\n",
    "    return B, W\n",
    "\n",
    "\n",
    "def fisher_loss_from_pairs(\n",
    "    n: int,\n",
    "    pairs: List[Tuple[int, int]],\n",
    "    K_pairs: Dict[Tuple[int, int], float],\n",
    "    y: np.ndarray,\n",
    "    eps: float = 1e-6,\n",
    ") -> FisherLossOut:\n",
    "    \"\"\"Fisher loss using only the pairs we evaluated.\n",
    "\n",
    "    We accumulate ⟨K,B⟩ and ⟨K,W⟩ using pair entries (symmetrised), assuming\n",
    "    K_ii = 1 (pure-state fidelity). This supports mini-batch pair sampling.\n",
    "    \"\"\"\n",
    "    B, W = fisher_mats(y)\n",
    "    SB = 0.0\n",
    "    SW = 0.0\n",
    "\n",
    "    # Diagonals contribute to W: tr(K P_c) = number of points in each class\n",
    "    # and to nothing in B beyond the -1/n term already absorbed in B.\n",
    "    # We'll add diagonal contributions analytically where applicable.\n",
    "    # Sum off-diagonals from provided pairs.\n",
    "    used = set()\n",
    "    for (i, j), v in K_pairs.items():\n",
    "        if i == j:\n",
    "            continue\n",
    "        SB += B[i, j] * v + B[j, i] * v\n",
    "        SW += W[i, j] * v + W[j, i] * v\n",
    "        used.add((i, j)); used.add((j, i))\n",
    "\n",
    "    # Add diagonal contributions\n",
    "    for i in range(n):\n",
    "        SB += B[i, i] * 1.0\n",
    "        SW += W[i, i] * 1.0\n",
    "\n",
    "    loss = -math.log(max(SB, 1e-12)) + math.log(SW + eps)\n",
    "    return FisherLossOut(loss=loss, SB=SB, SW=SW, B_mat=B, W_mat=W)\n",
    "\n",
    "\n",
    "# ------------------------------ Trainer --------------------------------- #\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    shots: int = 4096\n",
    "    batch_size: int = 16\n",
    "    max_iters: int = 200\n",
    "    lr: float = 0.1\n",
    "    grad_params_per_step: Optional[int] = None  # None → all params\n",
    "    seed: int = 7\n",
    "\n",
    "\n",
    "class FisherFidelityTrainer:\n",
    "    def __init__(self, cfg: FeatureMapConfig, train_cfg: TrainConfig):\n",
    "        self.cfg = cfg\n",
    "        self.train_cfg = train_cfg\n",
    "        self.kernel = FidelityKernel(cfg, shots=train_cfg.shots)\n",
    "        self.rng = random.Random(train_cfg.seed)\n",
    "\n",
    "    # ---- Pair selection ---- #\n",
    "    def sample_pairs(self, idxs: List[int], y: np.ndarray,\n",
    "                      intra_per_class: int = 4,\n",
    "                      inter_pairs: int = 16) -> List[Tuple[int, int]]:\n",
    "        pairs: List[Tuple[int, int]] = []\n",
    "        # Intra-class pairs\n",
    "        for c in np.unique(y[idxs]):\n",
    "            class_idx = [i for i in idxs if y[i] == c]\n",
    "            self.rng.shuffle(class_idx)\n",
    "            for k in range(min(intra_per_class, max(0, len(class_idx) - 1))):\n",
    "                pairs.append((class_idx[k], class_idx[-k - 1]))\n",
    "        # Inter-class pairs\n",
    "        others = idxs.copy()\n",
    "        self.rng.shuffle(others)\n",
    "        for _ in range(inter_pairs):\n",
    "            i, j = self.rng.sample(others, 2)\n",
    "            if y[i] != y[j]:\n",
    "                pairs.append((i, j))\n",
    "        # Deduplicate & sort tuples to keep (i,j) with i<=j for consistency\n",
    "        norm = []\n",
    "        seen = set()\n",
    "        for (i, j) in pairs:\n",
    "            a, b = (i, j) if i <= j else (j, i)\n",
    "            if (a, b) not in seen and a != b:\n",
    "                norm.append((a, b))\n",
    "                seen.add((a, b))\n",
    "        return norm\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> Tuple[float, Dict[str, float]]:\n",
    "        n = len(X)\n",
    "        # Select a mini-batch of indices\n",
    "        idxs = list(range(n))\n",
    "        self.rng.shuffle(idxs)\n",
    "        idxs = idxs[: self.train_cfg.batch_size]\n",
    "\n",
    "        # Sample informative pairs from the batch\n",
    "        pairs = self.sample_pairs(idxs, y)\n",
    "\n",
    "        # 1) Forward: fidelities on sampled pairs\n",
    "        K_pairs = self.kernel.fidelities(pairs, X, theta)\n",
    "\n",
    "        # 2) Loss + dL/dK coefficients\n",
    "        out = fisher_loss_from_pairs(n, pairs, K_pairs, y)\n",
    "        # Gradient wrt K is: -B/SB + W/(SW+eps)\n",
    "        dL_dK = -out.B_mat / max(out.SB, 1e-12) + out.W_mat / (out.SW + 1e-6)\n",
    "\n",
    "        # Restrict to pairs actually used\n",
    "        used_pairs = pairs\n",
    "\n",
    "        # 3) Parameter-shift gradients of K_ij\n",
    "        if self.train_cfg.grad_params_per_step is None:\n",
    "            param_indices = list(range(len(theta)))\n",
    "        else:\n",
    "            param_indices = self.rng.sample(list(range(len(theta))),\n",
    "                                            k=min(self.train_cfg.grad_params_per_step, len(theta)))\n",
    "\n",
    "        dK = {k: {} for k in param_indices}\n",
    "        for (i, j) in used_pairs:\n",
    "            for k in param_indices:\n",
    "                dK[k][(i, j)] = self.kernel.fidelity_param_shift_grad(X[i], X[j], theta, k)\n",
    "\n",
    "        # 4) Aggregate ∂L/∂θ_k = sum_{(i,j)} (dL/dK_ij) * (∂K_ij/∂θ_k) * sym_factor\n",
    "        # Symmetry factor: since we only keep i<j pairs, dL/dK contributes as B_ij and W_ij twice in ⟨K,·⟩.\n",
    "        grads = np.zeros_like(theta)\n",
    "        for k in param_indices:\n",
    "            s = 0.0\n",
    "            for (i, j), g_ij in dK[k].items():\n",
    "                coeff = dL_dK[i, j] + dL_dK[j, i]  # symmetric contribution\n",
    "                s += coeff * g_ij\n",
    "            grads[k] = s\n",
    "\n",
    "        # 5) Parameter update (SGD)\n",
    "        theta_new = theta - self.train_cfg.lr * grads\n",
    "\n",
    "        metrics = {\n",
    "            \"SB\": out.SB,\n",
    "            \"SW\": out.SW,\n",
    "            \"pairs\": float(len(used_pairs)),\n",
    "            \"grad_norm\": float(np.linalg.norm(grads)),\n",
    "        }\n",
    "        return float(out.loss), metrics, theta_new\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, theta0: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        if theta0 is None:\n",
    "            # Small random init to avoid symmetries\n",
    "            theta0 = 0.01 * np.random.default_rng(self.train_cfg.seed).standard_normal(\n",
    "                (4 * self.cfg.n_qubits + (self.cfg.n_qubits - 1)) * self.cfg.n_layers\n",
    "            )\n",
    "        theta = theta0.copy()\n",
    "\n",
    "        for it in range(1, self.train_cfg.max_iters + 1):\n",
    "            loss, metrics, theta = self.step(X, y, theta)\n",
    "            if it % 10 == 0 or it == 1:\n",
    "                print(f\"iter {it:4d} | loss={loss:.5f} | SB={metrics['SB']:.5f} | SW={metrics['SW']:.5f} | \"\n",
    "                      f\"pairs={int(metrics['pairs'])} | grad_norm={metrics['grad_norm']:.3f}\")\n",
    "        return theta\n",
    "\n",
    "\n",
    "# ------------------------------- Demo ----------------------------------- #\n",
    "def _toy_data(N: int = 40, d: int = 3, seed: int = 7) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Two interleaving blobs in R^d, labels 0/1.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x0 = rng.normal(loc=+1.0, scale=0.6, size=(N // 2, d))\n",
    "    x1 = rng.normal(loc=-1.0, scale=0.6, size=(N - N // 2, d))\n",
    "    X = np.vstack([x0, x1]).astype(float)\n",
    "    y = np.array([0] * (N // 2) + [1] * (N - N // 2), dtype=int)\n",
    "    # Shuffle\n",
    "    p = rng.permutation(N)\n",
    "    return X[p], y[p]\n",
    "\n",
    "\n",
    "def main_demo():\n",
    "    N, d = 24, 3\n",
    "    X, y = _toy_data(N=N, d=d)\n",
    "\n",
    "    cfg = FeatureMapConfig(n_qubits=3, data_dim=d, n_layers=2)\n",
    "    tcfg = TrainConfig(shots=2048, batch_size=16, max_iters=30, lr=0.3, grad_params_per_step=12)\n",
    "    trainer = FisherFidelityTrainer(cfg, tcfg)\n",
    "\n",
    "    theta = trainer.fit(X, y)\n",
    "\n",
    "    # Evaluate full-kernel Fisher after training (dense, for demo)\n",
    "    # Warning: O(N^2) echo calls; okay for small N.\n",
    "    pairs = [(i, j) for i in range(N) for j in range(i + 1, N)]\n",
    "    K_pairs = trainer.kernel.fidelities(pairs, X, theta)\n",
    "    out = fisher_loss_from_pairs(N, pairs, K_pairs, y)\n",
    "    print(\"\\nFinal Fisher stats: SB=\", out.SB, \"SW=\", out.SW, \"Loss=\", out.loss)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_demo()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
