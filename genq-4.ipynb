{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614cfe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santana/MyStuff/genq/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# End-to-end trainable quantum-kernel SVM with Qiskit + PyTorch\n",
    "# - Learns SVM params (beta, b) and quantum feature map params theta\n",
    "# - Backprop: parameter-shift through the kernel into the circuit\n",
    "import math, numpy as np, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import Statevector\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# === Save processed dataset, splits, and pipeline ===\n",
    "import os, json\n",
    "import joblib\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feccd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Set the path to the file you'd like to load\n",
    "# file_path = \"Synthetic_Financial_datasets_log.csv\"\n",
    "\n",
    "# # Load the latest version\n",
    "# df = kagglehub.load_dataset(\n",
    "#   KaggleDatasetAdapter.PANDAS,\n",
    "#   \"sriharshaeedala/financial-fraud-detection-dataset\",\n",
    "#   file_path,\n",
    "#   # Provide any additional arguments like \n",
    "#   # sql_query or pandas_kwargs. See the \n",
    "#   # documenation for more information:\n",
    "#   # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    "# )\n",
    "\n",
    "# print(\"First 5 records:\", df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e550ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === DataFrame-in / DataFrame-out cleaner ===\n",
    "# import re, numpy as np, pandas as pd\n",
    "# from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# # -------- helpers --------\n",
    "# def _snake(s: str) -> str:\n",
    "#     s = re.sub(r\"[^\\w]+\", \"_\", str(s).strip())\n",
    "#     s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "#     return s.lower()\n",
    "\n",
    "# def standardize_colnames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     return df.rename(columns={c: _snake(c) for c in df.columns})\n",
    "\n",
    "# def coerce_numeric_strings(s: pd.Series) -> pd.Series:\n",
    "#     x = s.astype(str).str.strip()\n",
    "#     x = x.str.replace(r\"\\(([^)]+)\\)\", r\"-\\1\", regex=True)  # \"(1,234)\" -> -1234\n",
    "#     x = x.str.replace(r\"[^\\d\\.\\-eE]\", \"\", regex=True)      # remove currency, commas, spaces\n",
    "#     return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "# def maybe_parse_datetime(col: pd.Series) -> Tuple[bool, Dict[str, pd.Series]]:\n",
    "#     dt = pd.to_datetime(col, errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
    "#     if dt.notna().mean() < 0.5:\n",
    "#         return False, {}\n",
    "#     feats = {\n",
    "#         \"year\": dt.dt.year.astype(\"Int64\"),\n",
    "#         \"month\": dt.dt.month.astype(\"Int8\"),\n",
    "#         \"day\": dt.dt.day.astype(\"Int8\"),\n",
    "#         \"dayofweek\": dt.dt.dayofweek.astype(\"Int8\"),\n",
    "#         \"hour\": dt.dt.hour.fillna(0).astype(\"Int8\"),\n",
    "#         \"dayofyear\": dt.dt.dayofyear.astype(\"Int16\"),\n",
    "#         \"is_month_end\": dt.dt.is_month_end.astype(\"Int8\"),\n",
    "#         \"is_month_start\": dt.dt.is_month_start.astype(\"Int8\"),\n",
    "#         \"is_weekend\": dt.dt.dayofweek.isin([5,6]).astype(\"Int8\"),\n",
    "#         \"epoch_seconds\": (dt.view(\"int64\") // 10**9).astype(\"Int64\"),\n",
    "#     }\n",
    "#     return True, feats\n",
    "\n",
    "# def drop_obvious_ids(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "#     id_like = [c for c in df.columns if re.search(r\"(?:^|_)(id|uuid|guid|hash|account|acct|customer|merchant)(?:_|$)\", c)]\n",
    "#     return df.drop(columns=id_like, errors=\"ignore\"), id_like\n",
    "\n",
    "# def detect_label(df: pd.DataFrame, hint: Optional[str]) -> Optional[str]:\n",
    "#     if hint and hint in df.columns: return hint\n",
    "#     for c in [\"is_fraud\",\"fraud\",\"fraud_bool\",\"fraudulent\",\"class\",\"target\",\"label\",\"y\"]:\n",
    "#         if c in df.columns: return c\n",
    "#     for c in df.columns:\n",
    "#         if \"fraud\" in c and df[c].nunique(dropna=True) <= 2:\n",
    "#             return c\n",
    "#     return None\n",
    "\n",
    "# def split_feature_types(df: pd.DataFrame, max_onehot: int = 30) -> Dict[str, List[str]]:\n",
    "#     types = {\"numeric\": [], \"boolean\": [], \"cat_low\": [], \"cat_high\": []}\n",
    "#     for c in df.columns:\n",
    "#         s = df[c]\n",
    "#         if pd.api.types.is_bool_dtype(s) or (s.dropna().isin([0,1,True,False]).all() and s.nunique(dropna=True) <= 2):\n",
    "#             types[\"boolean\"].append(c)\n",
    "#         elif pd.api.types.is_numeric_dtype(s):\n",
    "#             types[\"numeric\"].append(c)\n",
    "#         else:\n",
    "#             nun = s.nunique(dropna=True)\n",
    "#             (types[\"cat_low\"] if nun <= max_onehot else types[\"cat_high\"]).append(c)\n",
    "#     return types\n",
    "\n",
    "# # -------- main: DataFrame -> cleaned DataFrame --------\n",
    "# def clean_to_dataframe(\n",
    "#     df_raw: pd.DataFrame,\n",
    "#     label_hint: Optional[str] = None,\n",
    "#     max_onehot_cardinality: int = 30,\n",
    "#     clip_quantiles: Tuple[float,float] = (0.001, 0.999),\n",
    "#     scale_numeric: bool = False,    # set True to z-score numeric columns\n",
    "#     keep_label: bool = True         # append label back as last column if detected\n",
    "# ) -> pd.DataFrame:\n",
    "#     df = standardize_colnames(df_raw).copy()\n",
    "#     df = df.drop_duplicates()\n",
    "\n",
    "#     # Parse object columns: datetimes or numeric-like strings; else leave for categorical\n",
    "#     new_cols = {}\n",
    "#     drop_cols = []\n",
    "#     for c in list(df.columns):\n",
    "#         if pd.api.types.is_object_dtype(df[c]):\n",
    "#             ok_dt, feats = maybe_parse_datetime(df[c])\n",
    "#             if ok_dt:\n",
    "#                 for k, v in feats.items():\n",
    "#                     new_cols[f\"{c}__{k}\"] = v\n",
    "#                 drop_cols.append(c)\n",
    "#             else:\n",
    "#                 coerced = coerce_numeric_strings(df[c])\n",
    "#                 if coerced.notna().mean() >= 0.5:\n",
    "#                     df[c] = coerced\n",
    "\n",
    "#     if new_cols:\n",
    "#         for k, v in new_cols.items():\n",
    "#             df[k] = v\n",
    "#     if drop_cols:\n",
    "#         df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "#     # Bool-like strings → 0/1\n",
    "#     for c in df.select_dtypes(include=\"object\").columns:\n",
    "#         lc = df[c].str.lower()\n",
    "#         mask_bool = lc.isin([\"true\",\"false\",\"yes\",\"no\",\"y\",\"n\",\"t\",\"f\"])\n",
    "#         if mask_bool.mean() > 0.8:\n",
    "#             df[c] = lc.map({\"true\":1,\"false\":0,\"yes\":1,\"no\":0,\"y\":1,\"n\":0,\"t\":1,\"f\":0}).astype(\"Int8\")\n",
    "\n",
    "#     # Drop obvious IDs\n",
    "#     df, dropped_ids = drop_obvious_ids(df)\n",
    "\n",
    "#     # Detect & sanitize label\n",
    "#     y_name = detect_label(df, label_hint)\n",
    "#     y = None\n",
    "#     if y_name:\n",
    "#         y = df[y_name].copy()\n",
    "#         if not pd.api.types.is_numeric_dtype(y):\n",
    "#             if y.dropna().isin([0,1]).all():\n",
    "#                 y = y.astype(\"Int8\")\n",
    "#             elif y.nunique(dropna=True) == 2:\n",
    "#                 y = pd.Series(pd.Categorical(y).codes, index=y.index).astype(\"Int8\")\n",
    "#             else:\n",
    "#                 y = pd.Series(pd.Categorical(y).codes, index=y.index).astype(\"Int16\")\n",
    "#         df = df.drop(columns=[y_name])\n",
    "\n",
    "#     # Fill missing in remaining object columns so encoders work\n",
    "#     for c in df.select_dtypes(include=\"object\").columns:\n",
    "#         df[c] = df[c].fillna(\"<<missing>>\")\n",
    "\n",
    "#     # Split types\n",
    "#     types = split_feature_types(df, max_onehot_cardinality)\n",
    "\n",
    "#     # Impute numerics with median, clip outliers\n",
    "#     if types[\"numeric\"]:\n",
    "#         num_med = df[types[\"numeric\"]].median()\n",
    "#         df[types[\"numeric\"]] = df[types[\"numeric\"]].fillna(num_med)\n",
    "#         q_lo = df[types[\"numeric\"]].quantile(clip_quantiles[0])\n",
    "#         q_hi = df[types[\"numeric\"]].quantile(clip_quantiles[1])\n",
    "#         df[types[\"numeric\"]] = np.clip(df[types[\"numeric\"]], q_lo, q_hi, axis=1)\n",
    "\n",
    "#     # Impute booleans with mode and cast to Int8\n",
    "#     for c in types[\"boolean\"]:\n",
    "#         mode = df[c].mode(dropna=True)\n",
    "#         fillv = int(mode.iloc[0]) if not mode.empty else 0\n",
    "#         df[c] = df[c].fillna(fillv).astype(\"Int8\")\n",
    "\n",
    "#     # One-hot encode low-card categoricals\n",
    "#     if types[\"cat_low\"]:\n",
    "#         dummies = pd.get_dummies(\n",
    "#             df[types[\"cat_low\"]].astype(\"category\"),\n",
    "#             prefix=[c for c in types[\"cat_low\"]],\n",
    "#             prefix_sep=\"__\",\n",
    "#             dtype=\"Int8\"\n",
    "#         )\n",
    "#         df = df.drop(columns=types[\"cat_low\"]).join(dummies)\n",
    "\n",
    "#     # Ordinal encode high-card categoricals\n",
    "#     for c in types[\"cat_high\"]:\n",
    "#         s = df[c].astype(\"category\")\n",
    "#         df[f\"{c}__ordinal\"] = s.cat.codes.astype(\"Int32\")  # stable integer codes\n",
    "#     if types[\"cat_high\"]:\n",
    "#         df = df.drop(columns=types[\"cat_high\"])\n",
    "\n",
    "#     # Optional scaling of numeric columns (z-score), booleans untouched\n",
    "#     if scale_numeric and types[\"numeric\"]:\n",
    "#         num_cols = [c for c in df.columns if c in types[\"numeric\"]]\n",
    "#         df[num_cols] = (df[num_cols] - df[num_cols].mean()) / df[num_cols].std(ddof=0)\n",
    "\n",
    "#     # Append label at the end if requested\n",
    "#     if keep_label and y is not None:\n",
    "#         df[y_name] = y.values\n",
    "\n",
    "#     # Final guarantee: all numeric dtypes\n",
    "#     for c in df.columns:\n",
    "#         if pd.api.types.is_object_dtype(df[c]):\n",
    "#             # any leftover object (shouldn't happen) → category codes\n",
    "#             df[c] = df[c].astype(\"category\").cat.codes.astype(\"Int32\")\n",
    "\n",
    "#     print(f\"[clean] rows={len(df_raw)} → {len(df)} after dedupe\")\n",
    "#     print(f\"[clean] dropped ids: {dropped_ids}\")\n",
    "#     print(f\"[types] numeric={len(types['numeric'])}, boolean={len(types['boolean'])}, \"\n",
    "#           f\"cat_low={len(types['cat_low'])}, cat_high={len(types['cat_high'])}\")\n",
    "#     print(f\"[encode] final shape: {df.shape}, label: {y_name!r}\")\n",
    "#     return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd4592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- usage ----\n",
    "# clean_df = clean_to_dataframe(df, label_hint=None, max_onehot_cardinality=30, scale_numeric=False)\n",
    "# print(clean_df.dtypes.head())\n",
    "# print(clean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "060a9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"processed\", exist_ok=True)\n",
    "# clean_df.to_csv(\"processed/financial_fraud_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ed368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = pd.read_csv(\"processed/financial_fraud_processed.csv\")\n",
    "num = num.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78037f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (240, 14) | Test shape: (60, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example: ready for train/test split (uses detected label if present)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_name = \"isfraud\"\n",
    "if label_name is not None:\n",
    "    y = num[label_name].to_numpy()\n",
    "    X = num.drop(columns=[label_name]).to_numpy()\n",
    "else:\n",
    "    y = None\n",
    "    X = num.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y if y is not None else np.zeros(len(X)),  # dummy if no label\n",
    "    test_size=0.2, random_state=42, stratify=y if (y is not None and len(np.unique(y))>1) else None\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"| Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ad284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   1 | loss 5.0000 | test acc 98.9%\n",
      "step   2 | loss 9.6139 | test acc 98.9%\n",
      "step   3 | loss 4.2449 | test acc 98.9%\n",
      "step   4 | loss 3.8314 | test acc 23.3%\n",
      "step   5 | loss 5.9280 | test acc 56.7%\n",
      "step   6 | loss 5.0442 | test acc 98.9%\n",
      "step   7 | loss 2.7819 | test acc 98.9%\n",
      "step   8 | loss 2.4403 | test acc 98.9%\n",
      "step   9 | loss 3.7748 | test acc 98.9%\n",
      "step  10 | loss 3.3955 | test acc 98.9%\n",
      "step  11 | loss 1.9303 | test acc 98.9%\n",
      "step  12 | loss 1.5944 | test acc 98.9%\n",
      "step  13 | loss 2.3226 | test acc 98.9%\n",
      "step  14 | loss 2.1537 | test acc 98.9%\n",
      "step  15 | loss 1.1593 | test acc 98.9%\n",
      "step  16 | loss 1.1328 | test acc 98.9%\n",
      "step  17 | loss 1.6699 | test acc 98.9%\n",
      "step  18 | loss 1.5950 | test acc 98.9%\n",
      "step  19 | loss 0.9840 | test acc 98.9%\n",
      "step  20 | loss 0.4177 | test acc 98.9%\n",
      "step  21 | loss 1.0820 | test acc 98.9%\n",
      "step  22 | loss 1.1234 | test acc 98.9%\n",
      "step  23 | loss 0.1975 | test acc 98.9%\n",
      "step  24 | loss 0.3473 | test acc 98.9%\n",
      "step  25 | loss 0.4888 | test acc 98.9%\n",
      "step  26 | loss 0.4773 | test acc 98.9%\n",
      "step  27 | loss 0.3384 | test acc 98.9%\n",
      "step  28 | loss 0.1996 | test acc 98.9%\n",
      "step  29 | loss 0.1651 | test acc 98.9%\n",
      "step  30 | loss 0.4386 | test acc 98.9%\n",
      "step  31 | loss 0.1628 | test acc 98.9%\n",
      "step  32 | loss 0.1775 | test acc 98.9%\n",
      "step  33 | loss 0.2385 | test acc 98.9%\n",
      "step  34 | loss 0.2764 | test acc 98.9%\n",
      "step  35 | loss 0.2560 | test acc 98.9%\n",
      "step  36 | loss 0.2029 | test acc 98.9%\n",
      "step  37 | loss 0.1684 | test acc 98.9%\n",
      "step  38 | loss 0.1772 | test acc 98.9%\n",
      "step  39 | loss 0.2090 | test acc 98.9%\n",
      "step  40 | loss 0.2263 | test acc 98.9%\n",
      "step  41 | loss 0.2130 | test acc 98.9%\n",
      "step  42 | loss 0.1857 | test acc 98.9%\n",
      "step  43 | loss 0.1715 | test acc 98.9%\n",
      "step  44 | loss 0.1796 | test acc 98.9%\n",
      "step  45 | loss 0.1955 | test acc 98.9%\n",
      "step  46 | loss 0.1998 | test acc 98.9%\n",
      "step  47 | loss 0.1885 | test acc 98.9%\n",
      "step  48 | loss 0.1744 | test acc 98.9%\n",
      "step  49 | loss 0.1713 | test acc 98.9%\n",
      "step  50 | loss 0.1793 | test acc 98.9%\n",
      "step  51 | loss 0.1872 | test acc 98.9%\n",
      "step  52 | loss 0.1854 | test acc 98.9%\n",
      "step  53 | loss 0.1764 | test acc 98.9%\n",
      "step  54 | loss 0.1701 | test acc 98.9%\n",
      "step  55 | loss 0.1719 | test acc 98.9%\n",
      "step  56 | loss 0.1777 | test acc 98.9%\n",
      "step  57 | loss 0.1796 | test acc 98.9%\n",
      "step  58 | loss 0.1756 | test acc 98.9%\n",
      "step  59 | loss 0.1706 | test acc 98.9%\n",
      "step  60 | loss 0.1700 | test acc 98.9%\n",
      "step  61 | loss 0.1731 | test acc 98.9%\n",
      "step  62 | loss 0.1751 | test acc 98.9%\n",
      "step  63 | loss 0.1732 | test acc 98.9%\n",
      "step  64 | loss 0.1699 | test acc 98.9%\n",
      "step  65 | loss 0.1689 | test acc 98.9%\n",
      "step  66 | loss 0.1707 | test acc 98.9%\n",
      "step  67 | loss 0.1723 | test acc 98.9%\n",
      "step  68 | loss 0.1713 | test acc 98.9%\n",
      "step  69 | loss 0.1690 | test acc 98.9%\n",
      "step  70 | loss 0.1682 | test acc 98.9%\n",
      "step  71 | loss 0.1692 | test acc 98.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Toy data (binary labels ±1)\n",
    "# -----------------------------\n",
    "# X, y = make_moons(n_samples=1000, noise=0.9, random_state=0)\n",
    "X = StandardScaler().fit_transform(X).astype(np.float32)\n",
    "y = 2.0*y.astype(np.float32) - 1.0  # {0,1} -> {-1,+1}\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "Xtr = torch.tensor(Xtr)            # [n_train, d]\n",
    "Xte = torch.tensor(Xte)            # [n_test, d]\n",
    "ytr = torch.tensor(ytr)            # [n_train]\n",
    "yte = torch.tensor(yte)            # [n_test]\n",
    "\n",
    "n_train, d = Xtr.shape\n",
    "n_qubits = d\n",
    "n_layers = 2   # feel free to increase (cost grows with params × kernel evals)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) A parameterized Qiskit feature map  |phi_theta(x)>\n",
    "#    - Fixed data encoders (RX(x_i), RZ(x_i^2))\n",
    "#    - Trainable single-qubit layers + RZZ ring per layer\n",
    "#    All trainables are pure rotation angles -> clean shift rule.\n",
    "# ----------------------------------------------------\n",
    "@dataclass\n",
    "class AnsatzShape:\n",
    "    n_layers: int\n",
    "    n_qubits: int\n",
    "\n",
    "    @property\n",
    "    def sizes(self):\n",
    "        # For each layer and qubit: RY(theta_y), RZ(theta_z)\n",
    "        # And an RZZ entangler per (layer, qubit) along a ring\n",
    "        return {\n",
    "            \"theta_y\": (self.n_layers, self.n_qubits),\n",
    "            \"theta_z\": (self.n_layers, self.n_qubits),\n",
    "            \"theta_zz\": (self.n_layers, self.n_qubits),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def total_params(self):\n",
    "        L, Q = self.n_layers, self.n_qubits\n",
    "        return L * Q * 3\n",
    "\n",
    "def unflatten_theta(theta_flat: np.ndarray, shape: AnsatzShape):\n",
    "    L, Q = shape.n_layers, shape.n_qubits\n",
    "    assert theta_flat.size == shape.total_params\n",
    "    k = 0\n",
    "    th_y = theta_flat[k:k+L*Q].reshape(L, Q); k += L*Q\n",
    "    th_z = theta_flat[k:k+L*Q].reshape(L, Q); k += L*Q\n",
    "    th_zz = theta_flat[k:k+L*Q].reshape(L, Q)\n",
    "    return th_y, th_z, th_zz\n",
    "\n",
    "def build_feature_map(x_vec: np.ndarray, theta_flat: np.ndarray, shape: AnsatzShape) -> QuantumCircuit:\n",
    "    \"\"\"Return a circuit preparing |phi_theta(x)> from |0...0>.\"\"\"\n",
    "    L, Q = shape.n_layers, shape.n_qubits\n",
    "    th_y, th_z, th_zz = unflatten_theta(theta_flat, shape)\n",
    "\n",
    "    qc = QuantumCircuit(Q)\n",
    "    # Fixed data encoding\n",
    "    for q in range(Q):\n",
    "        qc.rx(float(x_vec[q]), q)     # RX(x_i)\n",
    "        qc.rz(float(x_vec[q]**2), q)  # RZ(x_i^2)\n",
    "\n",
    "    # Trainable layers\n",
    "    for l in range(L):\n",
    "        for q in range(Q):\n",
    "            qc.ry(float(th_y[l, q]), q)\n",
    "            qc.rz(float(th_z[l, q]), q)\n",
    "        # Entangling ring\n",
    "        for q in range(Q):\n",
    "            r = (q + 1) % Q\n",
    "            qc.rzz(float(th_zz[l, q]), q, r)\n",
    "    return qc\n",
    "\n",
    "def statevector_from_circuit(qc: QuantumCircuit) -> np.ndarray:\n",
    "    return Statevector.from_instruction(qc).data  # complex vector (2^n,)\n",
    "\n",
    "def batch_states(X_np: np.ndarray, theta_np: np.ndarray, shape: AnsatzShape) -> np.ndarray:\n",
    "    \"\"\"Return array [N, 2^n] of complex statevectors for all samples.\"\"\"\n",
    "    return np.stack([statevector_from_circuit(build_feature_map(x, theta_np, shape))\n",
    "                     for x in X_np], axis=0)\n",
    "\n",
    "def kernel_from_states(S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"K_ij = |<phi_i|phi_j>|^2 from S [N, D].\"\"\"\n",
    "    G = S @ S.conj().T             # [N,N] complex Gram\n",
    "    K = np.abs(G)**2               # fidelity kernel\n",
    "    # Stabilizer to keep PSD and help optimization:\n",
    "    K += 1e-6 * np.eye(K.shape[0], dtype=K.dtype)\n",
    "    return K.real.astype(np.float32)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Autograd: Quantum kernel forward + parameter-shift back\n",
    "# ---------------------------------------------------------\n",
    "class QKernelShift(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X: torch.Tensor, theta: torch.Tensor, n_layers_q: int):\n",
    "        # Save static config\n",
    "        shape = AnsatzShape(n_layers=int(n_layers_q), n_qubits=X.shape[1])\n",
    "        ctx.shape = shape\n",
    "\n",
    "        # Compute kernel K(theta) on CPU with Qiskit\n",
    "        X_np = X.detach().cpu().numpy()\n",
    "        th_np = theta.detach().cpu().numpy()\n",
    "        S = batch_states(X_np, th_np, shape)\n",
    "        K = kernel_from_states(S)\n",
    "\n",
    "        # For backward\n",
    "        ctx.save_for_backward(X.detach(), theta.detach())\n",
    "        return torch.from_numpy(K)  # [N,N] float32\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        # dL/dtheta = sum_ij (dL/dK_ij) * (dK_ij/dtheta)\n",
    "        X, theta = ctx.saved_tensors\n",
    "        shape: AnsatzShape = ctx.shape\n",
    "        X_np = X.cpu().numpy()\n",
    "        th_np = theta.cpu().numpy().copy()\n",
    "        gK = grad_output.detach().cpu().numpy()\n",
    "\n",
    "        shift = math.pi/2\n",
    "        P = th_np.size\n",
    "        grad_theta = np.zeros_like(th_np, dtype=np.float64)\n",
    "\n",
    "        # Helper to compute K for a given theta\n",
    "        def K_of(theta_vec: np.ndarray) -> np.ndarray:\n",
    "            S = batch_states(X_np, theta_vec, shape)\n",
    "            return kernel_from_states(S)\n",
    "\n",
    "        # Precompute K at current theta? Not necessary for central diff, but optional.\n",
    "\n",
    "        # Parameter-shift loop (exact for single-parameter rotation gates)\n",
    "        for p in range(P):\n",
    "            th_plus = th_np.copy();   th_plus[p]  += shift\n",
    "            th_minus = th_np.copy();  th_minus[p] -= shift\n",
    "\n",
    "            Kp = K_of(th_plus)\n",
    "            Km = K_of(th_minus)\n",
    "            dK = 0.5*(Kp - Km)       # (f(theta+π/2) - f(theta-π/2))/2\n",
    "\n",
    "            grad_theta[p] = np.sum(gK * dK, dtype=np.float64)\n",
    "\n",
    "        # No gradients for X (not learning the raw inputs)\n",
    "        grad_X = None\n",
    "        return grad_X, torch.from_numpy(grad_theta.astype(np.float32)), None\n",
    "\n",
    "def quantum_kernel_matrix(X: torch.Tensor, theta: torch.Tensor, n_layers_q: int) -> torch.Tensor:\n",
    "    return QKernelShift.apply(X, theta, n_layers_q)  # [N,N]\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4) SVM-in-primal parameters (β in R^n, b∈R)\n",
    "# --------------------------------------------\n",
    "shape = AnsatzShape(n_layers=n_layers, n_qubits=n_qubits)\n",
    "theta = torch.nn.Parameter(0.3*torch.randn(shape.total_params))  # trainable feature map\n",
    "beta = torch.nn.Parameter(torch.zeros(n_train))                  # SVM coeffs\n",
    "b = torch.nn.Parameter(torch.zeros(()))                          # bias\n",
    "\n",
    "C = 5.0     # soft-margin weight\n",
    "lr = 0.05\n",
    "optim = torch.optim.Adam([theta, beta, b], lr=lr)\n",
    "\n",
    "def svm_primal_loss(K: torch.Tensor, y: torch.Tensor, beta: torch.Tensor, b: torch.Tensor, C: float):\n",
    "    # scores s = K β + b\n",
    "    s = K @ beta + b                  # [n]\n",
    "    hinge = torch.clamp(1.0 - y * s, min=0.0)\n",
    "    reg = 0.5 * (beta @ (K @ beta))   # 0.5 * β^T K β\n",
    "    return reg + C * hinge.mean(), s\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5) Training loop (backprop end-to-end)\n",
    "# -----------------------------------------\n",
    "for step in range(200):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    K_tr = quantum_kernel_matrix(Xtr, theta, n_layers)  # backpropagates via parameter-shift\n",
    "    loss, scores = svm_primal_loss(K_tr, ytr, beta, b, C)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if (step+1) % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Evaluate cross-kernel K(Xte, Xtr) to classify test points\n",
    "            # (no gradient needed)\n",
    "            # Build states once per set for efficiency\n",
    "            Xtr_np = Xtr.cpu().numpy()\n",
    "            Xte_np = Xte.cpu().numpy()\n",
    "            th_np = theta.detach().cpu().numpy()\n",
    "            S_tr = batch_states(Xtr_np, th_np, shape)\n",
    "            S_te = batch_states(Xte_np, th_np, shape)\n",
    "            G_te_tr = S_te @ S_tr.conj().T\n",
    "            K_te_tr = (np.abs(G_te_tr)**2).astype(np.float32)\n",
    "\n",
    "            preds = np.sign(K_te_tr @ beta.detach().cpu().numpy() + b.detach().cpu().numpy())\n",
    "            acc = (preds.squeeze() == yte.cpu().numpy()).mean()\n",
    "        print(f\"step {step+1:3d} | loss {loss.item():.4f} | test acc {acc*100:.1f}%\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
